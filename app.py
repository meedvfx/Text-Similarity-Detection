import streamlit as st
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import string
import os # <-- AJOUTER CET IMPORT

# --- D√âBUT DE LA NOUVELLE SOLUTION ---

# 1. D√©finir un chemin local pour les donn√©es NLTK
NLTK_DATA_DIR = os.path.join(os.getcwd(), "nltk_data")

# 2. Cr√©er le dossier s'il n'existe pas
if not os.path.exists(NLTK_DATA_DIR):
    os.makedirs(NLTK_DATA_DIR)

# 3. Dire √† NLTK de TOUJOURS chercher les donn√©es ici
nltk.data.path.append(NLTK_DATA_DIR)

@st.cache_resource  # Met en cache cette fonction
def download_nltk_resources(download_dir):
    """T√©l√©charge les paquets NLTK requis dans un dossier sp√©cifique."""
    try:
        # Sp√©cifier le dossier de t√©l√©chargement !
        nltk.download('punkt', download_dir=download_dir)
        nltk.download('stopwords', download_dir=download_dir)
        nltk.download('wordnet', download_dir=download_dir)
        print(f"T√©l√©chargement NLTK r√©ussi dans {download_dir}")
        return True
    except Exception as e:
        print(f"Erreur lors du t√©l√©chargement NLTK : {e}")
        return False

# Ex√©cute la fonction de t√©l√©chargement au d√©marrage
NLTK_READY = download_nltk_resources(NLTK_DATA_DIR)

# --- FIN DE LA NOUVELLE SOLUTION ---

def preprocess_text(text):
    """
    Applique le pr√©traitement au texte :
    1. Minuscules
    2. Tokenisation
    3. Suppression de la ponctuation
    4. Suppression des stopwords
    5. Lemmatisation
    """
    # 1. Minuscules
    text_lower = text.lower()

    # 2. Tokenisation
    tokens = word_tokenize(text_lower)

    # 3. Suppression de la ponctuation
    tokens = [w for w in tokens if w not in string.punctuation]

    # 4. Suppression des stopwords (anglais par d√©faut, changez pour 'french')
    stop_words = set(stopwords.words('english'))
    cleaned_tokens = [w for w in tokens if w not in stop_words and w.isalnum()]

    # 5. Lemmatisation
    lemmatizer = WordNetLemmatizer()
    lemmatized_tokens = [lemmatizer.lemmatize(t) for t in cleaned_tokens]

    return " ".join(lemmatized_tokens)


st.set_page_config(page_title="D√©tecteur de Similarit√©", layout="wide")
st.title("üîé D√©tecteur de Similarit√© de Texte (Plagiat)")
st.write("Bas√© sur TF-IDF et la Similarit√© Cosinus")

# --- AJOUTER CETTE V√âRIFICATION ---
if NLTK_READY:
    # Cr√©er deux colonnes pour les bo√Ætes de texte
    col1, col2 = st.columns(2)

    with col1:
        st.header("Texte 1")
        text1 = st.text_area("Collez votre premier texte ici :", height=300, key="txt1")

    with col2:
        st.header("Texte 2")
        text2 = st.text_area("Collez votre deuxi√®me texte ici :", height=300, key="txt2")

    # Bouton pour lancer le calcul
    if st.button("Calculer la Similarit√©", type="primary"):
        if text1.strip() and text2.strip():
            # 1. Pr√©traitement du texte
            st.write("Pr√©traitement en cours...")
            proc_text1 = preprocess_text(text1)
            proc_text2 = preprocess_text(text2)

            documents = [proc_text1, proc_text2]

            # 2. Vectorisation (TF-IDF)
            st.write("Vectorisation (TF-IDF)...")
            vectorizer = TfidfVectorizer()
            tfidf_matrix = vectorizer.fit_transform(documents)

            # 3. Mod√©lisation (Calcul de la similarit√© cosinus)
            st.write("Calcul de la similarit√© cosinus...")
            # On compare le vecteur 0 (texte 1) au vecteur 1 (texte 2)
            cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])

            # Le r√©sultat est une matrice, on prend le premier (et seul) √©l√©ment
            similarity_score = cosine_sim[0][0]

            # Affichage des r√©sultats
            st.divider()
            st.subheader("R√©sultats")

            # Formater le score en pourcentage
            score_percent = similarity_score * 100

            st.metric(
                label="Score de Similarit√©",
                value=f"{score_percent:.2f} %"
            )

            st.progress(similarity_score)

            if similarity_score > 0.8:
                st.error("üö® **Alerte :** Similarit√© tr√®s √©lev√©e. Risque de plagiat.")
            elif similarity_score > 0.5:
                st.warning("‚ö†Ô∏è **Avertissement :** Similarit√© notable. Les textes partagent un vocabulaire commun.")
            else:
                st.success("‚úÖ **OK :** Les textes semblent diff√©rents.")

        else:
            st.warning("Veuillez entrer du texte dans les deux bo√Ætes.")

else:
    # Si NLTK n'a pas pu se t√©l√©charger, afficher une erreur
    st.error("Erreur critique : L'application n'a pas pu t√©l√©charger les ressources NLTK n√©cessaires pour fonctionner.")
    st.stop()