import streamlit as st
import re
import pypdf  # BibliothÃ¨que pour lire les PDF
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import SentenceTransformer, util

# --- GESTION DE L'Ã‰TAT (SESSION STATE) ---
# On initialise les variables pour stocker le texte si elles n'existent pas encore
if 'text1_content' not in st.session_state:
    st.session_state.text1_content = ""
if 'text2_content' not in st.session_state:
    st.session_state.text2_content = ""


# --- FONCTION D'EXTRACTION PDF ---
def extract_text_from_pdf(uploaded_file):
    """Extrait le texte brut d'un fichier PDF."""
    try:
        pdf_reader = pypdf.PdfReader(uploaded_file)
        text = ""
        for page in pdf_reader.pages:
            # On extrait le texte de chaque page et on l'ajoute
            text += page.extract_text() + "\n"
        return text
    except Exception as e:
        st.error(f"Erreur lors de la lecture du PDF : {e}")
        return ""


# --- CALLBACKS POUR L'UPLOAD ---
# Ces fonctions sont appelÃ©es dÃ¨s qu'un fichier est chargÃ©
def update_text1_from_pdf():
    uploaded_file = st.session_state.pdf1_uploader
    if uploaded_file is not None:
        text = extract_text_from_pdf(uploaded_file)
        st.session_state.text1_content = text


def update_text2_from_pdf():
    uploaded_file = st.session_state.pdf2_uploader
    if uploaded_file is not None:
        text = extract_text_from_pdf(uploaded_file)
        st.session_state.text2_content = text


# --- FONCTION DE PRÃ‰TRAITEMENT ---
def preprocess_text(text):
    text_lower = text.lower()
    text_cleaned = re.sub(r'[^a-z\s]', '', text_lower)
    text_cleaned = re.sub(r'\s+', ' ', text_cleaned).strip()
    return text_cleaned


# --- CHARGEMENT MODÃˆLE S-BERT ---
@st.cache_resource
def load_sbert_model():
    model = SentenceTransformer('all-MiniLM-L6-v2')
    return model


sbert_model = load_sbert_model()

# --- INTERFACE ---
st.set_page_config(page_title="DÃ©tecteur de SimilaritÃ©", layout="wide")
st.title("ğŸ” DÃ©tecteur de SimilaritÃ© de Texte (Plagiat)")
st.write("Comparez deux textes par copier-coller ou en important des fichiers PDF.")

st.divider()

# --- 1. CHOIX DU MODÃˆLE ---
st.header("1. Choisissez votre modÃ¨le")
model_choice = st.radio(
    "SÃ©lectionnez la mÃ©thode d'analyse :",
    ('TF-IDF', 'Sentence-BERT (S-BERT)', 'LSTM'),
    horizontal=True
)

ngram_tuple = (1, 1)
if model_choice == 'TF-IDF':
    st.subheader("Options TF-IDF")
    ngram_max = st.selectbox(
        "Taille maximale des N-grams :",
        (1, 2, 3, 4),
        format_func=lambda x: f"{x} (jusqu'Ã  {x}-grams)" if x > 1 else f"{x} (mots seuls)"
    )
    ngram_tuple = (1, ngram_max)

st.divider()

# --- 2. ENTRÃ‰E DES TEXTES (AVEC UPLOAD PDF) ---
st.header("2. Importez ou collez vos textes")
col1, col2 = st.columns(2)

with col1:
    st.subheader("Document 1")
    # Uploader de fichier (dÃ©clenche la mise Ã  jour du texte)
    st.file_uploader(
        "Importer un PDF (optionnel)",
        type="pdf",
        key="pdf1_uploader",
        on_change=update_text1_from_pdf
    )
    # Zone de texte (liÃ©e Ã  la variable 'text1_content' du session state)
    text1 = st.text_area(
        "Contenu du texte 1 :",
        height=300,
        key="text1_content"  # La clÃ© lie ce widget Ã  st.session_state.text1_content
    )

with col2:
    st.subheader("Document 2")
    st.file_uploader(
        "Importer un PDF (optionnel)",
        type="pdf",
        key="pdf2_uploader",
        on_change=update_text2_from_pdf
    )
    text2 = st.text_area(
        "Contenu du texte 2 :",
        height=300,
        key="text2_content"  # La clÃ© lie ce widget Ã  st.session_state.text2_content
    )

st.divider()

# --- 3. CALCUL ---
if st.button("Calculer la SimilaritÃ©", type="primary"):

    # On rÃ©cupÃ¨re le contenu directement depuis les zones de texte
    # (qui peuvent avoir Ã©tÃ© remplies par le PDF ou manuellement)
    content1 = text1.strip()
    content2 = text2.strip()

    if not (content1 and content2):
        st.warning("Veuillez fournir du texte pour les deux documents.")

    else:
        if model_choice == 'TF-IDF':
            st.subheader(f"RÃ©sultats (TF-IDF : {ngram_tuple})")
            try:
                proc_text1 = preprocess_text(content1)
                proc_text2 = preprocess_text(content2)
                documents = [proc_text1, proc_text2]

                vectorizer = TfidfVectorizer(ngram_range=ngram_tuple)
                tfidf_matrix = vectorizer.fit_transform(documents)

                cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])
                similarity_score = cosine_sim[0][0]

                st.divider()
                st.metric("Score de SimilaritÃ©", f"{similarity_score * 100:.2f} %")
                st.progress(similarity_score)

                if similarity_score > 0.8:
                    st.error("ğŸš¨ Risque Ã©levÃ© de plagiat.")
                elif similarity_score > 0.5:
                    st.warning("âš ï¸ SimilaritÃ© notable.")
                else:
                    st.success("âœ… Textes diffÃ©rents.")

            except ValueError:
                st.warning("Erreur : Textes vides aprÃ¨s nettoyage.")

        elif model_choice == 'Sentence-BERT (S-BERT)':
            st.subheader("RÃ©sultats (Sentence-BERT)")

            documents = [content1, content2]
            embeddings = sbert_model.encode(documents)
            cosine_sim = util.pytorch_cos_sim(embeddings[0], embeddings[1])
            similarity_score = cosine_sim[0][0].item()

            st.divider()
            st.metric("Score SÃ©mantique", f"{similarity_score * 100:.2f} %")
            st.progress(similarity_score)

            if similarity_score > 0.8:
                st.error("ğŸš¨ Sens trÃ¨s proche.")
            elif similarity_score > 0.5:
                st.warning("âš ï¸ Sens similaire.")
            else:
                st.success("âœ… Sens diffÃ©rent.")

        elif model_choice == 'LSTM':
            st.info("ğŸš§ ModÃ¨le LSTM en cours de dÃ©veloppement.")